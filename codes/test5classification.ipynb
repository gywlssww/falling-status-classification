{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/tarahjjeon/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:469: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/tarahjjeon/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:470: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/tarahjjeon/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:471: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/tarahjjeon/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:472: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/tarahjjeon/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:473: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/tarahjjeon/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:476: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import vstack,hstack,dstack,stack\n",
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import to_categorical\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# load a single file as a numpy array\n",
    "def load_file(filepath):\n",
    "    dataframe = read_csv(filepath)\n",
    "    #return dataframe.values\n",
    "    return dataframe\n",
    "\n",
    "def load_cols(filepath):\n",
    "    dataframe = read_csv(filepath)\n",
    "    #return dataframe.values\n",
    "    loaded=list()\n",
    "    dloaded=list()\n",
    "    for i in range(0,46806,234):\n",
    "        \n",
    "        for axis in ['x','y','z']:\n",
    "            loaded.append(dataframe[axis].loc[i:i+234].values)\n",
    "        loaded=stack(loaded,axis=1)\n",
    "        dloaded=dstack(loaded)\n",
    "    return dloaded,dataframe['label'].values\n",
    "# load a list of files and return as a 3d numpy array\n",
    "\n",
    "def load_group(filenames, prefix=''):\n",
    "    loaded = list()\n",
    "    for name in filenames:\n",
    "        data = load_file(prefix + name)\n",
    "        loaded.append(data)\n",
    "    # stack group so that features are the 3rd dimension\n",
    "    loaded = dstack(loaded)\n",
    "    return loaded\n",
    "\n",
    "# load a dataset group, such as train or test\n",
    "def load_dataset_group(group, prefix=''):\n",
    "    filepath = prefix + group + '/Inertial Signals/'\n",
    "    # load all 9 files as a single array\n",
    "    filenames = list()\n",
    "    # total acceleration\n",
    "    filenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n",
    "    # body acceleration\n",
    "    filenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt']\n",
    "    # body gyroscope\n",
    "    filenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n",
    "    # load input data\n",
    "    X = load_group(filenames, filepath)\n",
    "    # load class output\n",
    "    y = load_file(prefix + group + '/y_'+group+'.txt')\n",
    "    return X, y\n",
    "\n",
    "# load the dataset, returns train and test X and y elements\n",
    "def load_dataset(prefix=''):\n",
    "# load all train\n",
    "    trainX, trainy = load_dataset_group('train', prefix + 'HARDataset/')\n",
    "    print(trainX.shape, trainy.shape)\n",
    "    # load all test\n",
    "    testX, testy = load_dataset_group('test', prefix + 'HARDataset/')\n",
    "    print(testX.shape, testy.shape)\n",
    "    # zero-offset class values\n",
    "    trainy = trainy - 1\n",
    "    testy = testy - 1\n",
    "    # one hot encode y\n",
    "    trainy = to_categorical(trainy)\n",
    "    testy = to_categorical(testy)\n",
    "    print(trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
    "    return trainX, trainy, testX, testy\n",
    "\n",
    "# fit and evaluate a model\n",
    "def evaluate_model(trainX, trainy, testX, testy,verbose, epochs, batch_size):\n",
    "    #verbose, epochs, batch_size = 1, 50, 32\n",
    "    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, input_shape=(n_timesteps,n_features)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # fit network\n",
    "    model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    # evaluate model\n",
    "    _, train_acc = model.evaluate(trainX, trainy,batch_size=batch_size, verbose=0)\n",
    "    \n",
    "    _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "    return  epochs, batch_size,accuracy,train_acc\n",
    "\n",
    "# summarize scores\n",
    "def summarize_results(scores):\n",
    "    print(scores)\n",
    "    m, s = mean(scores), std(scores)\n",
    "    print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\n",
    "\n",
    "# run an experiment\n",
    "def run_experiment(trainX, trainy, testX, testy,batch):\n",
    "    # load data\n",
    "    ##trainX, trainy, testX, testy = load_dataset()\n",
    "    \n",
    "    # repeat experiment\n",
    "    scores = list()\n",
    "    for r,epoch in enumerate([40,50,60,100,120,150,200]):\n",
    "        epochs, batch_size,testscore,trainacc = evaluate_model(trainX, trainy, testX, testy,0,epoch,batch)\n",
    "        testscore = testscore * 100.0\n",
    "        trainacc=trainacc*100.0\n",
    "        print( \"epoch:\",epochs,\"/batch_size:\", batch_size)\n",
    "        print('test>#%d: %.3f' % (r+1, testscore))\n",
    "        print('train>#%d: %.3f' % (r+1, trainacc))\n",
    "        scores.append(testscore)\n",
    "    # summarize results\n",
    "    summarize_results(scores)\n",
    "\n",
    "    # run the experiment\n",
    "    #run_experiment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 40 /batch_size: 117\n",
      "test>#1: 34.000\n",
      "train>#1: 43.000\n",
      "epoch: 50 /batch_size: 117\n",
      "test>#2: 36.000\n",
      "train>#2: 41.500\n",
      "epoch: 60 /batch_size: 117\n",
      "test>#3: 42.000\n",
      "train>#3: 43.500\n",
      "epoch: 100 /batch_size: 117\n",
      "test>#4: 34.000\n",
      "train>#4: 45.500\n",
      "epoch: 120 /batch_size: 117\n",
      "test>#5: 46.000\n",
      "train>#5: 50.000\n",
      "epoch: 150 /batch_size: 117\n",
      "test>#6: 36.000\n",
      "train>#6: 43.000\n",
      "epoch: 200 /batch_size: 117\n",
      "test>#7: 42.000\n",
      "train>#7: 54.500\n",
      "[34.00000035762787, 36.000001430511475, 41.999998688697815, 34.00000035762787, 46.00000083446503, 36.000001430511475, 41.999998688697815]\n",
      "Accuracy: 38.571% (+/-4.371)\n",
      "epoch: 40 /batch_size: 234\n",
      "test>#1: 38.000\n",
      "train>#1: 39.500\n",
      "epoch: 50 /batch_size: 234\n",
      "test>#2: 36.000\n",
      "train>#2: 43.000\n",
      "epoch: 60 /batch_size: 234\n",
      "test>#3: 34.000\n",
      "train>#3: 42.000\n"
     ]
    }
   ],
   "source": [
    "run_experiment(trainxlist,train_y,testxlist,test_y,117) #dropout 0.2 nodes 100 \n",
    "run_experiment(trainxlist,train_y,testxlist,test_y,234) #dropout 0.2 nodes 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 40 /batch_size: 234\n",
      "test>#1: 36.000\n",
      "train>#1: 41.500\n",
      "epoch: 50 /batch_size: 234\n",
      "test>#2: 38.000\n",
      "train>#2: 39.500\n",
      "epoch: 60 /batch_size: 234\n",
      "test>#3: 36.000\n",
      "train>#3: 36.000\n",
      "epoch: 100 /batch_size: 234\n",
      "test>#4: 32.000\n",
      "train>#4: 48.000\n",
      "epoch: 120 /batch_size: 234\n",
      "test>#5: 32.000\n",
      "train>#5: 51.000\n",
      "epoch: 150 /batch_size: 234\n",
      "test>#6: 34.000\n",
      "train>#6: 47.000\n",
      "epoch: 200 /batch_size: 234\n",
      "test>#7: 36.000\n",
      "train>#7: 53.000\n",
      "[36.000001430511475, 37.99999952316284, 36.000001430511475, 31.999999284744263, 31.999999284744263, 34.00000035762787, 36.000001430511475]\n",
      "Accuracy: 34.857% (+/-2.100)\n"
     ]
    }
   ],
   "source": [
    "run_experiment(trainxlist,train_y,testxlist,test_y,234) #dropout 0.2 nodes 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20 /batch_size: 13\n",
      "test>#1: 40.000\n",
      "train>#1: 41.500\n",
      "epoch: 30 /batch_size: 13\n",
      "test>#2: 34.000\n",
      "train>#2: 45.500\n",
      "epoch: 40 /batch_size: 13\n",
      "test>#3: 32.000\n",
      "train>#3: 44.000\n",
      "epoch: 50 /batch_size: 13\n",
      "test>#4: 30.000\n",
      "train>#4: 52.500\n",
      "epoch: 60 /batch_size: 13\n",
      "test>#5: 32.000\n",
      "train>#5: 50.500\n",
      "epoch: 100 /batch_size: 13\n",
      "test>#6: 40.000\n",
      "train>#6: 50.000\n",
      "[40.0000015348196, 34.000000566244125, 32.0000007301569, 30.000000476837158, 32.0000005364418, 40.00000101327896]\n",
      "Accuracy: 34.667% (+/-3.944)\n",
      "epoch: 20 /batch_size: 39\n",
      "test>#1: 32.000\n",
      "train>#1: 42.500\n",
      "epoch: 30 /batch_size: 39\n",
      "test>#2: 40.000\n",
      "train>#2: 40.000\n",
      "epoch: 40 /batch_size: 39\n",
      "test>#3: 38.000\n",
      "train>#3: 46.500\n",
      "epoch: 50 /batch_size: 39\n",
      "test>#4: 36.000\n",
      "train>#4: 45.000\n",
      "epoch: 60 /batch_size: 39\n",
      "test>#5: 36.000\n",
      "train>#5: 49.500\n",
      "epoch: 100 /batch_size: 39\n",
      "test>#6: 36.000\n",
      "train>#6: 52.500\n",
      "[32.00000020861626, 40.00000008940697, 38.00000149011612, 35.99999997019768, 36.00000160932541, 36.00000160932541]\n",
      "Accuracy: 36.333% (+/-2.427)\n",
      "epoch: 20 /batch_size: 117\n",
      "test>#1: 40.000\n",
      "train>#1: 39.500\n",
      "epoch: 30 /batch_size: 117\n",
      "test>#2: 36.000\n",
      "train>#2: 42.000\n",
      "epoch: 40 /batch_size: 117\n",
      "test>#3: 42.000\n",
      "train>#3: 45.000\n",
      "epoch: 50 /batch_size: 117\n",
      "test>#4: 38.000\n",
      "train>#4: 50.500\n",
      "epoch: 60 /batch_size: 117\n",
      "test>#5: 44.000\n",
      "train>#5: 47.000\n",
      "epoch: 100 /batch_size: 117\n",
      "test>#6: 36.000\n",
      "train>#6: 51.500\n",
      "[40.00000059604645, 36.000001430511475, 41.999998688697815, 37.99999952316284, 43.99999976158142, 36.000001430511475]\n",
      "Accuracy: 39.333% (+/-2.981)\n",
      "epoch: 20 /batch_size: 234\n",
      "test>#1: 34.000\n",
      "train>#1: 40.500\n",
      "epoch: 30 /batch_size: 234\n",
      "test>#2: 36.000\n",
      "train>#2: 44.500\n",
      "epoch: 40 /batch_size: 234\n",
      "test>#3: 40.000\n",
      "train>#3: 44.500\n",
      "epoch: 50 /batch_size: 234\n",
      "test>#4: 32.000\n",
      "train>#4: 47.500\n",
      "epoch: 60 /batch_size: 234\n",
      "test>#5: 34.000\n",
      "train>#5: 47.500\n",
      "epoch: 100 /batch_size: 234\n",
      "test>#6: 36.000\n",
      "train>#6: 56.500\n",
      "[34.00000035762787, 36.000001430511475, 40.00000059604645, 31.999999284744263, 34.00000035762787, 36.000001430511475]\n",
      "Accuracy: 35.333% (+/-2.494)\n"
     ]
    }
   ],
   "source": [
    "run_experiment(trainxlist,train_y,testxlist,test_y,13) #dropout 0.2 nodes 100 \n",
    "run_experiment(trainxlist,train_y,testxlist,test_y,39) #dropout 0.2 nodes 100 \n",
    "run_experiment(trainxlist,train_y,testxlist,test_y,117) #dropout 0.2 nodes 100 \n",
    "run_experiment(trainxlist,train_y,testxlist,test_y,234) #dropout 0.2 nodes 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 40 /batch_size: 39\n",
      "test>#1: 38.000\n",
      "train>#1: 44.000\n",
      "epoch: 50 /batch_size: 39\n",
      "test>#2: 38.000\n",
      "train>#2: 44.000\n",
      "epoch: 60 /batch_size: 39\n",
      "test>#3: 30.000\n",
      "train>#3: 41.000\n",
      "[38.00000101327896, 38.000000685453415, 30.00000125169754]\n",
      "Accuracy: 35.333% (+/-3.771)\n"
     ]
    }
   ],
   "source": [
    "run_experiment(trainxlist,train_y,testxlist,test_y,13) #dropout 0.2 nodes 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 40 /batch_size: 39\n",
      "test>#1: 28.000\n",
      "train>#1: 39.500\n",
      "epoch: 50 /batch_size: 39\n",
      "test>#2: 36.000\n",
      "train>#2: 42.000\n",
      "epoch: 60 /batch_size: 39\n",
      "test>#3: 34.000\n",
      "train>#3: 43.500\n",
      "[28.00000038743019, 35.99999997019768, 34.00000137090683]\n",
      "Accuracy: 32.667% (+/-3.399)\n"
     ]
    }
   ],
   "source": [
    "run_experiment(trainxlist,train_y,testxlist,test_y,39) #dropout 0.2 nodes 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 40 /batch_size: 39\n",
      "test>#1: 40.000\n",
      "train>#1: 39.000\n",
      "epoch: 50 /batch_size: 39\n",
      "test>#2: 44.000\n",
      "train>#2: 40.000\n",
      "epoch: 60 /batch_size: 39\n",
      "test>#3: 34.000\n",
      "train>#3: 47.000\n",
      "[40.00000089406967, 44.00000065565109, 34.00000137090683]\n",
      "Accuracy: 39.333% (+/-4.110)\n"
     ]
    }
   ],
   "source": [
    "run_experiment(trainxlist,train_y,testxlist,test_y,117) #dropout 0.2 nodes 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 40 /batch_size: 39\n",
      "test>#1: 22.000\n",
      "train>#1: 42.500\n",
      "epoch: 50 /batch_size: 39\n",
      "test>#2: 40.000\n",
      "train>#2: 42.500\n",
      "epoch: 60 /batch_size: 39\n",
      "test>#3: 38.000\n",
      "train>#3: 45.500\n",
      "[22.000000566244125, 40.00000089406967, 38.00000065565109]\n",
      "Accuracy: 33.333% (+/-8.055)\n"
     ]
    }
   ],
   "source": [
    "run_experiment(trainxlist,train_y,testxlist,test_y,243) #dropout 0.2 nodes 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 40 /batch_size: 13\n",
      "test>#1: 38.000\n",
      "train>#1: 47.000\n",
      "epoch: 50 /batch_size: 13\n",
      "test>#2: 32.000\n",
      "train>#2: 45.000\n",
      "epoch: 60 /batch_size: 13\n",
      "test>#3: 36.000\n",
      "train>#3: 44.500\n",
      "[38.00000062584877, 32.00000110268593, 36.000001057982445]\n",
      "Accuracy: 35.333% (+/-2.494)\n"
     ]
    }
   ],
   "source": [
    "run_experiment(trainxlist,train_y,testxlist,test_y) #dropout 0.2 nodes 100 -> 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 40 /batch_size: 13\n",
      "test>#1: 40.000\n",
      "train>#1: 44.500\n",
      "epoch: 50 /batch_size: 13\n",
      "test>#2: 34.000\n",
      "train>#2: 48.000\n",
      "epoch: 60 /batch_size: 13\n",
      "test>#3: 30.000\n",
      "train>#3: 45.500\n",
      "[40.000000700354576, 34.00000098347664, 30.00000083446503]\n",
      "Accuracy: 34.667% (+/-4.110)\n"
     ]
    }
   ],
   "source": [
    "run_experiment(trainxlist,train_y,testxlist,test_y) #dropout 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 40 /batch_size: 13\n",
      "test>#1: 32.000\n",
      "train>#1: 44.000\n",
      "epoch: 50 /batch_size: 13\n",
      "test>#2: 40.000\n",
      "train>#2: 42.000\n",
      "epoch: 60 /batch_size: 13\n",
      "test>#3: 44.000\n",
      "train>#3: 41.000\n",
      "[32.00000062584877, 40.00000137090683, 44.00000149011612]\n",
      "Accuracy: 38.667% (+/-4.989)\n"
     ]
    }
   ],
   "source": [
    "run_experiment(trainxlist,train_y,testxlist,test_y) #dropout 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 40 /batch_size: 13\n",
      "test>#1: 34.000\n",
      "train>#1: 52.500\n",
      "epoch: 50 /batch_size: 13\n",
      "test>#2: 40.000\n",
      "train>#2: 46.000\n",
      "epoch: 60 /batch_size: 13\n",
      "test>#3: 38.000\n",
      "train>#3: 52.000\n",
      "epoch: 70 /batch_size: 13\n",
      "test>#4: 32.000\n",
      "train>#4: 44.000\n",
      "[34.00000075995922, 40.00000089406967, 38.00000110268593, 32.0000005364418]\n",
      "Accuracy: 36.000% (+/-3.162)\n"
     ]
    }
   ],
   "source": [
    "run_experiment(trainxlist,train_y,testxlist,test_y) #dropout 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 30 /batch_size: 39\n",
      "test>#1: 40.000\n",
      "train>#1: 44.500\n",
      "epoch: 50 /batch_size: 39\n",
      "test>#2: 40.000\n",
      "train>#2: 41.500\n",
      "epoch: 70 /batch_size: 39\n",
      "test>#3: 34.000\n",
      "train>#3: 48.000\n",
      "[40.00000137090683, 40.00000089406967, 34.000001192092896]\n",
      "Accuracy: 38.000% (+/-2.828)\n"
     ]
    }
   ],
   "source": [
    "run_experiment(trainxlist,train_y,testxlist,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 50 /batch_size: 64\n",
      "test>#1: 44.000\n",
      "train>#1: 47.500\n",
      "epoch: 70 /batch_size: 64\n",
      "test>#2: 38.000\n",
      "train>#2: 49.000\n",
      "epoch: 100 /batch_size: 64\n",
      "test>#3: 24.000\n",
      "train>#3: 44.000\n",
      "[43.99999976158142, 37.99999952316284, 23.999999463558197]\n",
      "Accuracy: 35.333% (+/-8.380)\n"
     ]
    }
   ],
   "source": [
    "run_experiment(trainxlist,train_y,testxlist,test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata=load_file(\"../data/train_data.csv\")\n",
    "testdata=load_file(\"../data/test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-333.0</td>\n",
       "      <td>9664.0</td>\n",
       "      <td>2624.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-2701.0</td>\n",
       "      <td>14208.0</td>\n",
       "      <td>5184.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-1677.0</td>\n",
       "      <td>14400.0</td>\n",
       "      <td>3392.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>50.0</td>\n",
       "      <td>17792.0</td>\n",
       "      <td>2368.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-2189.0</td>\n",
       "      <td>13696.0</td>\n",
       "      <td>1152.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       x        y       z  label\n",
       "0           0  -333.0   9664.0  2624.0      0\n",
       "1           1 -2701.0  14208.0  5184.0      0\n",
       "2           2 -1677.0  14400.0  3392.0      0\n",
       "3           3    50.0  17792.0  2368.0      0\n",
       "4           4 -2189.0  13696.0  1152.0      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata=load_file(\"../data/train_data.csv\").fillna(method='pad')\n",
    "testdata=load_file(\"../data/test_data.csv\").fillna(method='pad')\n",
    "trainxlist=list()\n",
    "trainylist=list()\n",
    "testylist=list()\n",
    "for i in range(0,46800,234):\n",
    "    t=traindata[['x','y','z']].iloc[i:i+234]\n",
    "    trainxlist.append(t)\n",
    "    trainylist.append(traindata['label'].iloc[i])\n",
    "try:\n",
    "    trainxlist=stack(trainxlist,axis=0)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 234, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainxlist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "testxlist=list()\n",
    "for i in range(0,11700,234):\n",
    "    t=testdata[['x','y','z']].iloc[i:i+234]\n",
    "    testxlist.append(t)\n",
    "    testylist.append(testdata['label'].iloc[i])\n",
    "\n",
    "try:\n",
    "    testxlist=stack(testxlist,axis=0)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 5)\n",
      "(200, 5)\n"
     ]
    }
   ],
   "source": [
    "#train_x,train_y=load_cols(\"../data/train_data.csv\")\n",
    "#test_x,test_y=load_cols(\"../data/test_data.csv\")\n",
    "train_y=to_categorical(trainylist)\n",
    "test_y=to_categorical(testylist)\n",
    "print(test_y.shape)\n",
    "print(train_y.shape) # (datalen,5classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score,accuracy_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "traindata=load_file(\"../data/train_data.csv\").fillna(method='pad')\n",
    "testdata=load_file(\"../data/test_data.csv\").fillna(method='pad')\n",
    "X_train=traindata[['x','y','z']]\n",
    "X_test=testdata[['x','y','z']]\n",
    "y_train=traindata['label']\n",
    "y_test=testdata['label']\n",
    "C = [.1,1,10,100]\n",
    "probability = [True]\n",
    "\n",
    "param_grid = [\n",
    "  {'C': C, 'kernel':['linear'], 'probability':probability}\n",
    "]\n",
    "\n",
    "# Create a base model\n",
    "svc = svm.SVC(decision_function_shape ='ovo', random_state=8)\n",
    "\n",
    "# Manually create the splits in CV in order to be able to fix a random_state (GridSearchCV doesn't have that argument)\n",
    "#cv_sets = ShuffleSplit(n_splits = 10, test_size = .13, random_state = 8)\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator=svc, \n",
    "                           param_grid=param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=5)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_svc = grid_search.best_estimator_\n",
    "svc_pred = best_svc.predict(X_test)\n",
    "print(grid_search.best_params_)\n",
    "y_hat=grid_search.predict(X_test)\n",
    "print(metrics.classification_report(y_test,y_hat,digits=4))\n",
    "print(\"--- %s seconds ---\"%(time.time()-start_time))\n",
    "print(\"----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
